library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
load("week13.RData")
glimpse(week13)
week13Tidy = week13 %>%
select(id, manager, describeManager) %>%
ungroup() %>%
mutate(describeManager = unlist(.$describeManager),
describeManager = gsub("^c\\(|\\)$", "", .$describeManager))
employeeSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
employeeSentiment
managerSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(manager, id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
group_by(manager) %>%
summarize(meanNegative = mean(negative),
meanPositive = mean(positive),
sentiment = meanPositive - meanNegative)
managerSentiment
worstManager = managerSentiment$manager[which.min(managerSentiment$sentiment)]
bestManager = managerSentiment$manager[which.max(managerSentiment$sentiment)]
week13$describeManager[week13$manager == worstManager][sample(1:5, 1)]
week13$describeManager[week13$manager == bestManager][sample(1:5, 1)]
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = hash_sentiment_jockers_rinker)
week13Tidy %>%
mutate(element_id = 1:nrow(.)) %>%
left_join(., vadarSent, by = "element_id") %>%
group_by(manager) %>%
summarize(meanSent = mean(sentiment))
shiny::runApp('Documents/ND MS Data Science/R/HelloWorld')
faithful[, 2]
runApp('Documents/ND MS Data Science/R/HelloWorld')
faithful
knitr::opts_chunk$set(echo = TRUE)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
#clear the work environment
rm(list=ls())
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(tidytext)
library(sentimentr)
library(lexicon)
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
glimpse(last_words)
View(last_words)
last_words <- last_words %>% select("inmateNumber", "correctedStatements")
last_words <- last_words %>%
drop_na() %>%
select("inmateNumber", "correctedStatements")
head(last_words)
head(last_words, 10)
last_words <- last_words %>%
drop_na() %>%
select("inmateNumber", "correctedStatements") %>%
unnest_tokens(word, correctedStatements) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
drop_na() %>%
select("inmateNumber", "correctedStatements") %>%
unnest_tokens(word, correctedStatements) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
#clear the work environment
rm(list=ls())
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
library(magrittr)
library(wordcloud2)
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
library(magrittr)
library(wordcloud2)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
drop_na()
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 10)
last_words_data <- get_sentences(last_words)
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words  )
last_words_data <- get_sentences(last_words)
last_words_dat
last_words_data <- get_sentences(last_words)
last_words_data
last_words <- gsub("\n", "", last_words  )
last_words_data <- get_sentences(last_words)
last_words_data
last_words_sent <- sentiment(last_words_data)
last_words_sent
head(last_words, 20)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements  )
last_words <- gsub("\n", "", last_words$correctedStatements  )
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements  )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words[$correctedStatements[,2] )
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements[,2] )
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- gsub("\n", "", last_words[,2]  )
last_words <- gsub("\n", "", last_words[,2]  )
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- gsub("\n", "", last_words[,2]  )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
Drop the NAs and select only the text columns
```{r}
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
```
Let's take a look at the data to see if there are any weird things going on.
```{r}
head(last_words, 20)
head(last_words, 20)
```{r}
#filter out special characters
last_words <- gsub("`", ".", last_words[,2] )
head(last_words, 20)
#filter out special characters
last_words <- gsub("`", ".", last_words[,2] )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
Drop the NAs and select only the text columns
```{r}
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
```
Let's take a look at the data to see if there are any weird things going on.
```{r}
head(last_words, 20)
head(last_words, 20)
```{r}
#filter out special characters
last_words[,2] <- gsub("`", ".", last_words[,2] )
last_words[,2] <- gsub("\n", "", last_words[,2]  )
head(last_words, 20)
last_words_data <- get_sentences(last_words)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words[,2] <- gsub("\x96", ".", last_words[,2] )
last_words[,2] <- gsub("\n", "", last_words[,2]  )
head(last_words, 20)
last_words_data <- get_sentences(last_words)
last_words_data <- get_sentences(last_words)
head(last_words, 37)
View(last_words, 37)
View(last_words, 37)
View(last_words)
last_words[37,2]
last_words[,2] <- gsub("\xfc\xbe\x8d\xb3\xa4\xbc", "", last_words[,2]  )
last_words[37,2]
last_words_data <- get_sentences(last_words)
last_words[105,2]
last_words[,2] <- gsub("\x85", ".", last_words[,2] )
last_words_data <- get_sentences(last_words)
last_words[425,2]
last_words[,2] <- gsub("\xa0", ".", last_words[,2] )
last_words_data <- get_sentences(last_words)
last_words_data
last_words_sent <- sentiment(last_words_data)
last_words_sent
last_words_sent[order(-sentiment),]
last_words_sent_by <- sentiment_by(last_words_data)
last_words_sent_by
last_words_sent_by[order(-ave_sentiment),]
last_words[last_words_data$element_id == 116,]
last_words[last_words_data$element_id == 60,]
last_words[last_words_data$element_id == 143,]
last_words[last_words_data$element_id == 15,]
last_words_dat %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
na.omit() %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
na.omit() %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
summary(last_words)
summary(last_words[,2])
str(last_words[,2])
last_words[,2] %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words[,2] %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
shiny::runApp('Documents/ND MS Data Science/R/HelloWorld')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
install.packages("shinythemes")
install.packages("shinythemes")
shiny::runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
knitr::opts_chunk$set(echo = TRUE)
topic3 = stm(documents = week15TextPrep$documents,
vocab = week15TextPrep$vocab,
K = 3, verbose = FALSE)
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
#load the required libraries
#install.packages("stm")
library(stm)
week15 = read.csv("week15-practice.csv", stringsAsFactors = FALSE)
week15Text = textProcessor(documents = week15$review,
metadata = week15)
rvest::guess_encoding(week15$review)
week15$review = iconv(week15$review, "ISO-8859-1", "UTF-8", sub = "")
week15$review = gsub("[^[:graph:]]|Ãƒ", " ", week15$review, perl = TRUE)
week15$review = iconv(week15$review, "ISO-8859-1", "UTF-8", sub = "")
nrow(week15[grepl("REDACTED", week15$review), ])
week15TextProcess = textProcessor(documents = week15$review,
metadata = week15,
onlycharacter = TRUE,
customstopwords = c("redacted",
tm::stopwords("SMART"),
tm::stopwords("en")))
week15TextPrep = prepDocuments(documents = week15TextProcess$documents,
vocab = week15TextProcess$vocab,
meta = week15TextProcess$meta)
topic3 = stm(documents = week15TextPrep$documents,
vocab = week15TextPrep$vocab,
K = 3, verbose = FALSE)
plot(topic3)
labelTopics(topic3)
findThoughts(topic3, texts = week15$review, n = 1)
checkResiduals(topic3, documents = week15TextPrep$documents)
topic4 = stm(documents = week15TextPrep$documents,
vocab = week15TextPrep$vocab,
K = 4, verbose = FALSE)
plot(topic4)
checkResiduals(topic4, documents = week15TextPrep$documents)
kTest = searchK(documents = week15TextPrep$documents,
vocab = week15TextPrep$vocab,
K = c(3, 4, 5, 10, 20, 25, 35), verbose = FALSE)
plot(kTest)
topic35 = stm(documents = week15TextPrep$documents,
vocab = week15TextPrep$vocab,
K = 35, verbose = FALSE)
plot(topic35)
checkResiduals(topic35, documents = week15TextPrep$documents)
knitr::opts_chunk$set(echo = TRUE)
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp")
library(sf)
library(sf)
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp")
library(sf)
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp")
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp")
View(abandoned.properties)
View(abandoned.properties)
schools <- st_read("School_Boundaries.shp")
View(schools)
View(schools)
str(abandoned.properties)
str(abandoned.properties$Outcome_St)
(abandoned.properties$Outcome_St)
unique(abandoned.properties$Outcome_St)
unique(abandoned.properties$Code_Enfor)
unique(abandoned.properties$Outcome_St)
schools <- st_read("School_Boundaries.shp", StringsAsFactors = FALSE)
#global variables
setwd("SouthBend") #for console only
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp", StringsAsFactors = FALSE)
street.lights <- read.csv("Street_Lights.csv")
parks <- read.csv("Public_Facilities.csv")
code.enforcement <- read.csv("Code_Enforcement_Cases.csv")
#global variables
setwd("SouthBend") #for console only
#global variables
setwd("SouthBend") #for console only
#global variables
setwd("DataVizSouthBend/SouthBend") #for console only
#global variables
setwd("DataVizSouthBend/SouthBend") #for console only
#global variables
setwd("Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Visualization/DataVizSouthBend/SouthBend") #for console only
#global variables
setwd("Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
#global variables
setwd("Users/Cindy") #for console only
#global variables
setwd("Users/cindy") #for console only
#global variables
setwd("users/cindy") #for console only
#global variables
setwd("Users/Cindy") #for console only
#global variables
setwd("/Users/Cindy") #for console only
setwd("/Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
#global variables
#setwd("/Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
setwd("SouthBend")
#global variables
#setwd("/Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
#setwd("SouthBend") #for console only
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp", StringsAsFactors = FALSE)
street.lights <- read.csv("Street_Lights.csv")
#global variables
#setwd("/Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
#setwd("SouthBend") #for console only
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp", StringsAsFactors = FALSE)
#global variables
#setwd("/Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
#setwd("SouthBend") #for console only
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp")
schools <- st_read("School_Boundaries.shp")
#for Cindy's code
abandoned.properties.no.nas <- abandoned.properties %>% drop_na()
library(dplyr)
#for Cindy's code
abandoned.properties.no.nas <- abandoned.properties %>% drop_na()
library(tidyverse)
#for Cindy's code
abandoned.properties.no.nas <- abandoned.properties %>% drop_na()
#for Cindy's code
abandoned.properties.no.nas <- unique(abandoned.properties$Outcome_St) %>% drop_na()
abandoned.properties.no.nas
#for Cindy's code
abandoned.properties.no.nas <- abandoned.properties %>% drop_na()
unique(abandoned.properties.no.nas)
abandoned.properties.no.nas <- abandoned.properties %>% drop_na()
unique(abandoned.properties.no.nas)
unique(abandoned.properties.no.nas$Outcome_St)
unique(abandoned.properties.no.nas$Outcome_St)
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
shiny::runApp()
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
#load the required libraries
library(tidyverse)
library(haven)
library(tidyr)
library(psych)
library(jsonlite)
library(ggcorrplot)
library(mirt)
library(factoextra)
library(cluster)
library(mclust)
library(NbClust)
library(fclust)
library(poLCA)
#read in the data
sfoDf <- read_delim('SFO_survey_withText.txt', delim="\t")
head(sfoDf)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(Q6A:Q6N) %>% drop_na()
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(., Q6A:Q6N) %>% drop_na()
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(Q6A:Q6N) %>% drop_na()
#read in the data
sfoDf <- read_delim('SFO_survey_withText.txt', delim="\t")
head(sfoDf)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(Q6A:Q6N) %>% drop_na()
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(Q6A:Q6N)
#read in the data
sfoDf <- read_delim('SFO_survey_withText.txt', delim="\t")
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(Q6A:Q6N)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(RESPNUM, Q6A:Q6N)
View(sfoDf)
View(sfoDf)
sfoDf$Q6A[sfoDf$Q6A == 0] = 6
sfoDf$Q6B[sfoDf$Q6B == 0] = 6
sfoDf$Q6C[sfoDf$Q6C == 0] = 6
sfoDf$Q6D[sfoDf$Q6D == 0] = 6
sfoDf$Q6E[sfoDf$Q6E == 0] = 6
sfoDf$Q6F[sfoDf$Q6F == 0] = 6
sfoDf$Q6G[sfoDf$Q6G == 0] = 6
sfoDf$Q6H[sfoDf$Q6H == 0] = 6
sfoDf$Q6I[sfoDf$Q6I == 0] = 6
sfoDf$Q6J[sfoDf$Q6J == 0] = 6
sfoDf$Q6K[sfoDf$Q6K == 0] = 6
sfoDf$Q6L[sfoDf$Q6L == 0] = 6
sfoDf$Q6M[sfoDf$Q6M == 0] = 6
sfoDf$Q6N[sfoDf$Q6N == 0] = 6
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(Q6A:Q6N)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select("Q6A:Q6N")
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(Q6A)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(Q6A)
#select question 6 questions we are interested in
sfoDf <- sfoDf
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% select(RESPNUM)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% dplyr::select(RESPNUM)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% dplyr::select(Q6A:Q6N)
#read in the data
sfoDf <- read_delim('SFO_survey_withText.txt', delim="\t")
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% dplyr::select(Q6A:Q6N)
sfoDf %>%
cor(., use="pairwise.complete") %>%
ggcorrplot()
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 4, maxiter = 10000)
lcaFormula = cbind(Q6A, Q6B, Q6C, Q6D, Q6E, Q6F, Q6G, Q6H, Q6I, Q6J, Q6K, Q6L, Q6M, Q6N, Q17, Q18, Q19) ~ 1
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 4, maxiter = 10000)
lcaFormula = cbind(Q6A, Q6B, Q6C, Q6D, Q6E, Q6F, Q6G, Q6H, Q6I, Q6J, Q6K, Q6L, Q6M, Q6N) ~ 1
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 4, maxiter = 10000)
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 3, maxiter = 10000)
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 5, maxiter = 10000)
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 3, maxiter = 10000)
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 4, maxiter = 10000)
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 3, maxiter = 10000)
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 5, maxiter = 10000)
max_II <- -100000
min_bic <- 100000
for(i in 2:10){
lc <- poLCA(lcaFormula, sfoDf, nclass=i, maxiter=3000,
tol=1e-5, na.rm=FALSE,
nrep=10, verbose=TRUE, calc.se=TRUE)
if(lc$bic < min_bic){
min_bic <- lc$bic
LCA_best_model<-lc
}
}
LCA_best_model
plot(lcaAllqsClasses)
(plot(lcaAllqsClasses)
plot(lcaAllqsClasses)
plot(lcaAllqsClasses)
