select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements  )
last_words <- gsub("\n", "", last_words$correctedStatements  )
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements  )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words[$correctedStatements[,2] )
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements[,2] )
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- gsub("\n", "", last_words[,2]  )
last_words <- gsub("\n", "", last_words[,2]  )
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- gsub("\n", "", last_words[,2]  )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
Drop the NAs and select only the text columns
```{r}
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
```
Let's take a look at the data to see if there are any weird things going on.
```{r}
head(last_words, 20)
head(last_words, 20)
```{r}
#filter out special characters
last_words <- gsub("`", ".", last_words[,2] )
head(last_words, 20)
#filter out special characters
last_words <- gsub("`", ".", last_words[,2] )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
Drop the NAs and select only the text columns
```{r}
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
```
Let's take a look at the data to see if there are any weird things going on.
```{r}
head(last_words, 20)
head(last_words, 20)
```{r}
#filter out special characters
last_words[,2] <- gsub("`", ".", last_words[,2] )
last_words[,2] <- gsub("\n", "", last_words[,2]  )
head(last_words, 20)
last_words_data <- get_sentences(last_words)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words[,2] <- gsub("\x96", ".", last_words[,2] )
last_words[,2] <- gsub("\n", "", last_words[,2]  )
head(last_words, 20)
last_words_data <- get_sentences(last_words)
last_words_data <- get_sentences(last_words)
head(last_words, 37)
View(last_words, 37)
View(last_words, 37)
View(last_words)
last_words[37,2]
last_words[,2] <- gsub("\xfc\xbe\x8d\xb3\xa4\xbc", "", last_words[,2]  )
last_words[37,2]
last_words_data <- get_sentences(last_words)
last_words[105,2]
last_words[,2] <- gsub("\x85", ".", last_words[,2] )
last_words_data <- get_sentences(last_words)
last_words[425,2]
last_words[,2] <- gsub("\xa0", ".", last_words[,2] )
last_words_data <- get_sentences(last_words)
last_words_data
last_words_sent <- sentiment(last_words_data)
last_words_sent
last_words_sent[order(-sentiment),]
last_words_sent_by <- sentiment_by(last_words_data)
last_words_sent_by
last_words_sent_by[order(-ave_sentiment),]
last_words[last_words_data$element_id == 116,]
last_words[last_words_data$element_id == 60,]
last_words[last_words_data$element_id == 143,]
last_words[last_words_data$element_id == 15,]
last_words_dat %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
na.omit() %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
na.omit() %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
summary(last_words)
summary(last_words[,2])
str(last_words[,2])
last_words[,2] %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words[,2] %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
shiny::runApp('Documents/ND MS Data Science/R/HelloWorld')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
install.packages("shinythemes")
install.packages("shinythemes")
shiny::runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
#global variables
setwd("/Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
shiny::runApp()
code.outcome.names
code.outcome.names
library(shiny)
#install.packages("shinythemes")
library(shinythemes)
library(sf)
library(leaflet)
library(tidyverse)
#global variables
#setwd("/Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
#setwd("SouthBend") #for console only
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp")
street.lights <- read.csv("Street_Lights.csv")
parks <- read.csv("Public_Facilities.csv")
code.enforcement <- read.csv("Code_Enforcement_Cases.csv")
schools <- st_read("School_Boundaries.shp")
#for Cindy's code
abandoned.properties.no.nas <- abandoned.properties %>% drop_na()
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
code.outcome.names
#global variables
#setwd("/Users/Cindy/Documents/ND\ MS\ Data\ Science/Data\ Viz/DataVizSouthBend/SouthBend") #for console only
#setwd("SouthBend") #for console only
abandoned.properties <- st_read("Abandoned_Property_Parcels.shp", stringsAsFactors = FALSE)
#for Cindy's code
abandoned.properties.no.nas <- abandoned.properties %>% drop_na()
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
code.outcome.names
#for Cindy's code
abandoned.properties.no.nas <- abandoned.properties
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
code.outcome.names
runApp()
View(abandoned.properties.no.nas)
View(abandoned.properties.no.nas)
schools.no.nas <- schools
schools.types <- unique(schools.no.nas$SchoolType)
schools.no.nas <- schools
schools.types <- unique(schools.no.nas$SchoolType)
schools.types
schools <- st_read("School_Boundaries.shp", stringsAsFactor)
schools <- st_read("School_Boundaries.shp", stringsAsFactors = FALSE)
schools.no.nas <- schools
schools.types <- unique(schools.no.nas$SchoolType)
schools.types
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
#load the required libraries
library(tidyverse)
library(haven)
library(tidyr)
library(psych)
library(jsonlite)
library(ggcorrplot)
library(mirt)
library(factoextra)
library(cluster)
library(mclust)
library(NbClust)
library(fclust)
library(poLCA)
#read in the data
sfoDf <- read_delim('SFO_survey_withText.txt', delim="\t")
#head(sfoDf)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% dplyr::select(Q6A:Q6N)
#head(sfoDf)
sfoDf %>%
cor(., use="pairwise.complete") %>%
ggcorrplot()
kmeansTest = kmeans(x = na.omit(sfoDf),
centers = 3)
fviz_cluster(kmeansTest, sfoDf)
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
#load the required libraries
library(tidyverse)
library(haven)
library(tidyr)
library(psych)
library(jsonlite)
library(ggcorrplot)
library(mirt)
library(factoextra)
library(cluster)
library(mclust)
library(NbClust)
library(fclust)
library(poLCA)
#read in the data
sfoDf <- read_delim('SFO_survey_withText.txt', delim="\t")
#head(sfoDf)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% dplyr::select(Q6A:Q6N) %>% drop_na()
#head(sfoDf)
sfoDf %>%
cor(., use="pairwise.complete") %>%
ggcorrplot()
kmeansTest = kmeans(x = na.omit(sfoDf),
centers = 3)
fviz_cluster(kmeansTest, sfoDf)
sfoDf$Q6A[sfoDf$Q6A == 0] = 6
sfoDf$Q6B[sfoDf$Q6B == 0] = 6
sfoDf$Q6C[sfoDf$Q6C == 0] = 6
sfoDf$Q6D[sfoDf$Q6D == 0] = 6
sfoDf$Q6E[sfoDf$Q6E == 0] = 6
sfoDf$Q6F[sfoDf$Q6F == 0] = 6
sfoDf$Q6G[sfoDf$Q6G == 0] = 6
sfoDf$Q6H[sfoDf$Q6H == 0] = 6
sfoDf$Q6I[sfoDf$Q6I == 0] = 6
sfoDf$Q6J[sfoDf$Q6J == 0] = 6
sfoDf$Q6K[sfoDf$Q6K == 0] = 6
sfoDf$Q6L[sfoDf$Q6L == 0] = 6
sfoDf$Q6M[sfoDf$Q6M == 0] = 6
sfoDf$Q6N[sfoDf$Q6N == 0] = 6
lcaFormula = cbind(Q6A, Q6B, Q6C, Q6D, Q6E, Q6F, Q6G, Q6H, Q6I, Q6J, Q6K, Q6L, Q6M, Q6N) ~ 1
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 5, maxiter = 10000)
plot(lcaAllqsClasses)
kmeansTest$centers
fviz_cluster(kmeansTest, sfoDf)
kmeansTest$centers
mean(kmeansTest$centers)
kmeansTest$centers
kmeansTest$centers[1,]
kmeansTest$centers
mean(kmeansTest$centers[1,])
mean(kmeansTest$centers[2,])
mean(kmeansTest$centers[3,])
kmeansTest$centers
mean(kmeansTest$centers[1,])
mean(kmeansTest$centers[2,])
mean(kmeansTest$centers[3,])
mean(kmeansTest$centers[2,])
kmeansTest$cluster
kmeansTest$cluster[1,]
kmeansTest$cluster[1]
kmeansTest$cluster[2]
kmeansTest$cluster[100]
kmeansTest$cluster[200]
kmeansTest$cluster[202]
kmeansTest$cluster[300]
kmeansTest$cluster[1]
kmeansTest$cluster[3]
kmeansTest$cluster[6]
kmeansTest$cluster[7]
kmeansTest$cluster[7]
sfoDf[7,]
sfoDf[100,]
sfoDf[300,]
kmeansTest$cluster[1]
sfoDf[1,]
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
#load the required libraries
library(tidyverse)
library(haven)
library(tidyr)
library(psych)
library(jsonlite)
library(ggcorrplot)
library(mirt)
library(factoextra)
library(cluster)
library(mclust)
library(NbClust)
library(fclust)
library(poLCA)
#read in the data
sfoDf <- read_delim('SFO_survey_withText.txt', delim="\t")
#head(sfoDf)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% dplyr::select(Q6A:Q6N) %>% drop_na()
#head(sfoDf)
sfoDf %>%
cor(., use="pairwise.complete") %>%
ggcorrplot()
kmeansTest = kmeans(x = na.omit(sfoDf),
centers = 3)
fviz_cluster(kmeansTest, sfoDf)
kmeansTest$centers
mean(kmeansTest$centers[1,])
mean(kmeansTest$centers[2,])
mean(kmeansTest$centers[3,])
kmeansTest$cluster[7]
sfoDf[7,]
kmeansTest$cluster[100]
sfoDf[100,]
kmeansTest$cluster[300]
sfoDf[300,]
sfoDf$Q6A[sfoDf$Q6A == 0] = 6
sfoDf$Q6B[sfoDf$Q6B == 0] = 6
sfoDf$Q6C[sfoDf$Q6C == 0] = 6
sfoDf$Q6D[sfoDf$Q6D == 0] = 6
sfoDf$Q6E[sfoDf$Q6E == 0] = 6
sfoDf$Q6F[sfoDf$Q6F == 0] = 6
sfoDf$Q6G[sfoDf$Q6G == 0] = 6
sfoDf$Q6H[sfoDf$Q6H == 0] = 6
sfoDf$Q6I[sfoDf$Q6I == 0] = 6
sfoDf$Q6J[sfoDf$Q6J == 0] = 6
sfoDf$Q6K[sfoDf$Q6K == 0] = 6
sfoDf$Q6L[sfoDf$Q6L == 0] = 6
sfoDf$Q6M[sfoDf$Q6M == 0] = 6
sfoDf$Q6N[sfoDf$Q6N == 0] = 6
lcaFormula = cbind(Q6A, Q6B, Q6C, Q6D, Q6E, Q6F, Q6G, Q6H, Q6I, Q6J, Q6K, Q6L, Q6M, Q6N) ~ 1
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 5, maxiter = 10000)
plot(lcaAllqsClasses)
set.seed(1847)
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
#load the required libraries
library(tidyverse)
library(haven)
library(tidyr)
library(psych)
library(jsonlite)
library(ggcorrplot)
library(mirt)
library(factoextra)
library(cluster)
library(mclust)
library(NbClust)
library(fclust)
library(poLCA)
set.seed(1847)
#read in the data
sfoDf <- read_delim('SFO_survey_withText.txt', delim="\t")
#head(sfoDf)
#select question 6 questions we are interested in
sfoDf <- sfoDf %>% dplyr::select(Q6A:Q6N) %>% drop_na()
#head(sfoDf)
sfoDf %>%
cor(., use="pairwise.complete") %>%
ggcorrplot()
kmeansTest = kmeans(x = na.omit(sfoDf),
centers = 3)
fviz_cluster(kmeansTest, sfoDf)
kmeansTest$centers
mean(kmeansTest$centers[1,])
mean(kmeansTest$centers[2,])
mean(kmeansTest$centers[3,])
kmeansTest$cluster[7]
sfoDf[7,]
kmeansTest$cluster[100]
sfoDf[100,]
kmeansTest$cluster[300]
sfoDf[300,]
sfoDf$Q6A[sfoDf$Q6A == 0] = 6
sfoDf$Q6B[sfoDf$Q6B == 0] = 6
sfoDf$Q6C[sfoDf$Q6C == 0] = 6
sfoDf$Q6D[sfoDf$Q6D == 0] = 6
sfoDf$Q6E[sfoDf$Q6E == 0] = 6
sfoDf$Q6F[sfoDf$Q6F == 0] = 6
sfoDf$Q6G[sfoDf$Q6G == 0] = 6
sfoDf$Q6H[sfoDf$Q6H == 0] = 6
sfoDf$Q6I[sfoDf$Q6I == 0] = 6
sfoDf$Q6J[sfoDf$Q6J == 0] = 6
sfoDf$Q6K[sfoDf$Q6K == 0] = 6
sfoDf$Q6L[sfoDf$Q6L == 0] = 6
sfoDf$Q6M[sfoDf$Q6M == 0] = 6
sfoDf$Q6N[sfoDf$Q6N == 0] = 6
lcaFormula = cbind(Q6A, Q6B, Q6C, Q6D, Q6E, Q6F, Q6G, Q6H, Q6I, Q6J, Q6K, Q6L, Q6M, Q6N) ~ 1
lcaAllqsClasses = poLCA(lcaFormula, sfoDf, nclass = 5, maxiter = 10000)
plot(lcaAllqsClasses)
kmeansTest$cluster[100]
sfoDf[100,]
kmeansTest$cluster[300]
sfoDf[300,]
kmeansTest$cluster[7]
sfoDf[7,]
shiny::runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
schools <- st_read("School_Boundaries/School_Boundaries.shp", stringsAsFactors = FALSE)
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
runApp()
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
#Assign the Icon
parkLocDf.spatial['icon'] <- sapply(strsplit(as.string(parkLocDf.spatial$Park_Type)," "), `[`, 1)
#Assign the Icon
parkLocDf.spatial['icon'] <- sapply(strsplit(as.character(parkLocDf.spatial$Park_Type)," "), `[`, 1)
runApp('~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
schools[schools$SchoolType %in% input$schools.types,]
schools <- st_read("School_Boundaries/School_Boundaries.shp", stringsAsFactors = FALSE)
setwd("~/Documents/ND MS Data Science/Data Viz/DataVizSouthBend")
schools <- st_read("School_Boundaries/School_Boundaries.shp", stringsAsFactors = FALSE)
schools[schools$SchoolType %in% input$schools.types,]
runApp()
schools[schools$SchoolType %in% c("Public","Private"),]
schools[schools$SchoolType %in% c("Public","Private"),]
runApp()
schools[schools$SchoolType %in% schools.types,]
schools[schools$SchoolType %in% schools.types,]
schools <- st_read("School_Boundaries/School_Boundaries.shp", stringsAsFactors = FALSE)
schools[schools$SchoolType %in% schools.types,]
schools.types <- unique(schools.no.nas$SchoolType)
schools.no.nas <- schools
schools.types <- unique(schools.no.nas$SchoolType)
schools[schools$SchoolType %in% schools.types,]
runApp()
code.outcome.names <- code.outcome.names[-4]
code.enforcement <- read.csv("Code_Enforcement_Cases.csv")
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
code.outcome.names <- code.outcome.names %>% select(-NA)
code.outcome.names
code.enforcement <- read.csv("Code_Enforcement_Cases.csv")
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
code.enforcement <- read.csv("Code_Enforcement_Cases.csv")
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
## GLOBAL VARIABLES/DATA
abandoned.properties <- st_read("Abandoned_Property_Parcels/Abandoned_Property_Parcels.shp", stringsAsFactors = FALSE)
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
code.outcome.names <- code.outcome.names %>% select(-NA)
code.outcome.names
code.outcome.names[-5]
code.outcome.names <- code.outcome.names %>% code.outcome.names[-5]
## CINDY'S CODE - START
abandoned.properties.no.nas <- abandoned.properties
code.outcome.names <- unique(abandoned.properties.no.nas$Outcome_St)
code.outcome.names <- code.outcome.names %>% code.outcome.names[-5]
code.outcome.names <- code.outcome.names[-5]
runApp()
addTiles()%>%
#setView(lng = 6.2520, lat = 41.6764, zoom = 14) %>%
addPolygons(data = abandoned.properties, color = ~pal(properties.subset()$Outcome_St), popup = properties.subset()$Direction) %>%
addPolygons(data = schools, color = ~school.pal(schools.subset()$SchoolType)) # %>%
runApp()
