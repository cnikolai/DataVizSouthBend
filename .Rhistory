summary(parks.spatial)
ggplot() +  geom_sf(data = parks.spatial, aes(color = Park_Type))
library("ggmap")
register_google(key = "AIzaSyAe2Brb2eyF0ZeyEncpk_36JQkC-o9Xyvg")
nikolai.points<- read.csv("Nikolai_Business_Licenses_30.csv", stringsAsFactors = FALSE)
nikolai.geo <- nikolai.points %>% mutate_geocode(location = Full_Address, output = "more")
View(nikolai.geo)
write.csv(nikolai.geo, file = "nikolai_week3.csv")
library(leaflet)
library(sf)
#read in the shape file
schools <- st_read("School_Boundaries.shp", stringsAsFactors = FALSE)
parks<- read.csv("Parks_Locations_and_Features.csv")
parks.spatial <- parks %>%
st_as_sf(coords  = c("Lon","Lat"))%>%
st_set_crs(value = 4326)
leaflet()  %>%
addTiles()  %>%
addPolygons(data = schools, popup = ~School_name) %>%
addMarkers(data = parks.spatial, popup = ~Park_Name)
leaflet()  %>%
addTiles()  %>%
addPolygons(data = schools, popup = ~School) %>%
addMarkers(data = parks.spatial, popup = ~Park_Name)
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
setwd("~/Documents/ND MS Data Science")
#load the libraries
library(leaflet)
library(sf)
#read in the shape file
schools <- st_read("School_Boundaries.shp", stringsAsFactors = FALSE)
View(schools)
parks<- read.csv("Parks_Locations_and_Features.csv")
parks.spatial <- parks %>%
st_as_sf(coords  = c("Lon","Lat"))%>%
st_set_crs(value = 4326)
leaflet()  %>%
addTiles()  %>%
addPolygons(data = schools, popup = ~School) %>%
addMarkers(data = parks.spatial, popup = ~Park_Name)
install.packages(xquartz)
install.packages(xquartz)
install.packages(XQuartz)
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
#setwd("~/Documents/ND MS Data Science")
#load the libraries
library(leaflet)
library(sf)
#read in the shape file
schools <- st_read("School_Boundaries.shp", stringsAsFactors = FALSE)
View(schools)
parks<- read.csv("Parks_Locations_and_Features.csv")
parks.spatial <- parks %>%
st_as_sf(coords  = c("Lon","Lat"))%>%
st_set_crs(value = 4326)
leaflet()  %>%
addTiles()  %>%
addPolygons(data = schools, popup = ~School) %>%
addMarkers(data = parks.spatial, popup = ~Park_Name)
knitr::opts_chunk$set(echo = TRUE)
#clear the environment
rm(list=ls())
#setwd("~/Documents/ND MS Data Science")
#load the libraries
library(leaflet)
library(sf)
#read in the shape file
schools <- st_read("School_Boundaries.shp", stringsAsFactors = FALSE)
parks<- read.csv("Parks_Locations_and_Features.csv")
parks.spatial <- parks %>%
st_as_sf(coords  = c("Lon","Lat"))%>%
st_set_crs(value = 4326)
leaflet()  %>%
addTiles()  %>%
addPolygons(data = schools, popup = ~School) %>%
addMarkers(data = parks.spatial, popup = ~Park_Name)
install.packages(tm)
install.packages(SnowballC)
install.packages("tm")
install.packages("SnowballC")
knitr::opts_chunk$set(echo = TRUE)
#clear the work environment
rm(list=ls())
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
load("week13.RData")
glimpse(week13)
week13Tidy = week13 %>%
select(id, manager, describeManager) %>%
ungroup() %>%
mutate(describeManager = unlist(.$describeManager),
describeManager = gsub("^c\\(|\\)$", "", .$describeManager))
employeeSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
employeeSentiment
managerSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(manager, id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
group_by(manager) %>%
summarize(meanNegative = mean(negative),
meanPositive = mean(positive),
sentiment = meanPositive - meanNegative)
managerSentiment
worstManager = managerSentiment$manager[which.min(managerSentiment$sentiment)]
bestManager = managerSentiment$manager[which.max(managerSentiment$sentiment)]
week13$describeManager[week13$manager == worstManager][sample(1:5, 1)]
week13$describeManager[week13$manager == bestManager][sample(1:5, 1)]
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon::hash_sentiment_vadar)
install.packages("lexicon")
install.packages("lexicon")
knitr::opts_chunk$set(echo = TRUE)
#clear the work environment
rm(list=ls())
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
load("week13.RData")
glimpse(week13)
week13Tidy = week13 %>%
select(id, manager, describeManager) %>%
ungroup() %>%
mutate(describeManager = unlist(.$describeManager),
describeManager = gsub("^c\\(|\\)$", "", .$describeManager))
employeeSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
employeeSentiment
managerSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(manager, id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
group_by(manager) %>%
summarize(meanNegative = mean(negative),
meanPositive = mean(positive),
sentiment = meanPositive - meanNegative)
managerSentiment
worstManager = managerSentiment$manager[which.min(managerSentiment$sentiment)]
bestManager = managerSentiment$manager[which.max(managerSentiment$sentiment)]
week13$describeManager[week13$manager == worstManager][sample(1:5, 1)]
week13$describeManager[week13$manager == bestManager][sample(1:5, 1)]
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon::hash_sentiment_vadar)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon:::hash_sentiment_vadar)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon::hash_sentiment_vadar)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon::hash_sentiment_vadar)
library(lexicon)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon::hash_sentiment_vadar)
shiny::runApp('Documents/ND MS Data Science/R/HelloWorld')
runApp('Documents/ND MS Data Science/R/HelloWorld')
install.packages('rsconnect')
install.packages("rsconnect")
rsconnect::setAccountInfo(name='cynthianikolai',
token='46A2335122EBB1DF37AE16D008D4D47B',
secret='S/eyi2lA90kQFb/aCCRp+O1s3Km/mzpAf1lgTZLC')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
knitr::opts_chunk$set(echo = TRUE)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = hash_sentiment_vadar)
vadarSent = lexicon::sentiment(week13Tidy$describeManager, polarity_dt = hash_sentiment_vadar)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon::hash_sentiment_vadar)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon::hash_sentiment_vadar)
knitr::opts_chunk$set(echo = TRUE)
#clear the work environment
rm(list=ls())
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
load("week13.RData")
glimpse(week13)
week13Tidy = week13 %>%
select(id, manager, describeManager) %>%
ungroup() %>%
mutate(describeManager = unlist(.$describeManager),
describeManager = gsub("^c\\(|\\)$", "", .$describeManager))
employeeSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
employeeSentiment
managerSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(manager, id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
group_by(manager) %>%
summarize(meanNegative = mean(negative),
meanPositive = mean(positive),
sentiment = meanPositive - meanNegative)
managerSentiment
worstManager = managerSentiment$manager[which.min(managerSentiment$sentiment)]
bestManager = managerSentiment$manager[which.max(managerSentiment$sentiment)]
week13$describeManager[week13$manager == worstManager][sample(1:5, 1)]
week13$describeManager[week13$manager == bestManager][sample(1:5, 1)]
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = lexicon::hash_sentiment_vadar)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = hash_sentiment_vadar)
testing <- data(hash_sentiment_vadar)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = hash_sentiment_vadar)
testing <- data(hash_sentiment_vadar)
testing <- data(lexicon::hash_sentiment_vadar)
```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("trinker/lexicon")
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = hash_sentiment_vadar)
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = hash_sentiment_jockers_rinker)
knitr::opts_chunk$set(echo = TRUE)
#clear the work environment
rm(list=ls())
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
load("week13.RData")
glimpse(week13)
week13Tidy = week13 %>%
select(id, manager, describeManager) %>%
ungroup() %>%
mutate(describeManager = unlist(.$describeManager),
describeManager = gsub("^c\\(|\\)$", "", .$describeManager))
employeeSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
employeeSentiment
managerSentiment = week13Tidy %>%
unnest_tokens(tbl = ., output = word, input = describeManager) %>%
group_by(manager, id) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
group_by(manager) %>%
summarize(meanNegative = mean(negative),
meanPositive = mean(positive),
sentiment = meanPositive - meanNegative)
managerSentiment
worstManager = managerSentiment$manager[which.min(managerSentiment$sentiment)]
bestManager = managerSentiment$manager[which.max(managerSentiment$sentiment)]
week13$describeManager[week13$manager == worstManager][sample(1:5, 1)]
week13$describeManager[week13$manager == bestManager][sample(1:5, 1)]
vadarSent = sentiment(week13Tidy$describeManager, polarity_dt = hash_sentiment_jockers_rinker)
week13Tidy %>%
mutate(element_id = 1:nrow(.)) %>%
left_join(., vadarSent, by = "element_id") %>%
group_by(manager) %>%
summarize(meanSent = mean(sentiment))
shiny::runApp('Documents/ND MS Data Science/R/HelloWorld')
faithful[, 2]
runApp('Documents/ND MS Data Science/R/HelloWorld')
faithful
knitr::opts_chunk$set(echo = TRUE)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
#clear the work environment
rm(list=ls())
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(tidytext)
library(sentimentr)
library(lexicon)
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
glimpse(last_words)
View(last_words)
last_words <- last_words %>% select("inmateNumber", "correctedStatements")
last_words <- last_words %>%
drop_na() %>%
select("inmateNumber", "correctedStatements")
head(last_words)
head(last_words, 10)
last_words <- last_words %>%
drop_na() %>%
select("inmateNumber", "correctedStatements") %>%
unnest_tokens(word, correctedStatements) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
drop_na() %>%
select("inmateNumber", "correctedStatements") %>%
unnest_tokens(word, correctedStatements) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
#clear the work environment
rm(list=ls())
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
library(magrittr)
library(wordcloud2)
#load the required libraries
library(dplyr)
library(tidyr)
library(tidytext)
library(sentimentr)
library(lexicon)
library(magrittr)
library(wordcloud2)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
drop_na()
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 10)
last_words_data <- get_sentences(last_words)
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words  )
last_words_data <- get_sentences(last_words)
last_words_dat
last_words_data <- get_sentences(last_words)
last_words_data
last_words <- gsub("\n", "", last_words  )
last_words_data <- get_sentences(last_words)
last_words_data
last_words_sent <- sentiment(last_words_data)
last_words_sent
head(last_words, 20)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements  )
last_words <- gsub("\n", "", last_words$correctedStatements  )
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements  )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words[$correctedStatements[,2] )
#filter out special characters
last_words <- gsub("\x96", "", last_words$correctedStatements[,2] )
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- gsub("\n", "", last_words[,2]  )
last_words <- gsub("\n", "", last_words[,2]  )
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words <- gsub("\x96", "", last_words[,2] )
last_words <- gsub("\n", "", last_words[,2]  )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
Drop the NAs and select only the text columns
```{r}
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
```
Let's take a look at the data to see if there are any weird things going on.
```{r}
head(last_words, 20)
head(last_words, 20)
```{r}
#filter out special characters
last_words <- gsub("`", ".", last_words[,2] )
head(last_words, 20)
#filter out special characters
last_words <- gsub("`", ".", last_words[,2] )
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
Drop the NAs and select only the text columns
```{r}
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
```
Let's take a look at the data to see if there are any weird things going on.
```{r}
head(last_words, 20)
head(last_words, 20)
```{r}
#filter out special characters
last_words[,2] <- gsub("`", ".", last_words[,2] )
last_words[,2] <- gsub("\n", "", last_words[,2]  )
head(last_words, 20)
last_words_data <- get_sentences(last_words)
last_words <- read.csv("BDS-W13-W15-txEx-DataSet.csv")
last_words <- last_words %>%
select("inmateNumber", "correctedStatements") %>%
drop_na()
head(last_words, 20)
#filter out special characters
last_words[,2] <- gsub("\x96", ".", last_words[,2] )
last_words[,2] <- gsub("\n", "", last_words[,2]  )
head(last_words, 20)
last_words_data <- get_sentences(last_words)
last_words_data <- get_sentences(last_words)
head(last_words, 37)
View(last_words, 37)
View(last_words, 37)
View(last_words)
last_words[37,2]
last_words[,2] <- gsub("\xfc\xbe\x8d\xb3\xa4\xbc", "", last_words[,2]  )
last_words[37,2]
last_words_data <- get_sentences(last_words)
last_words[105,2]
last_words[,2] <- gsub("\x85", ".", last_words[,2] )
last_words_data <- get_sentences(last_words)
last_words[425,2]
last_words[,2] <- gsub("\xa0", ".", last_words[,2] )
last_words_data <- get_sentences(last_words)
last_words_data
last_words_sent <- sentiment(last_words_data)
last_words_sent
last_words_sent[order(-sentiment),]
last_words_sent_by <- sentiment_by(last_words_data)
last_words_sent_by
last_words_sent_by[order(-ave_sentiment),]
last_words[last_words_data$element_id == 116,]
last_words[last_words_data$element_id == 60,]
last_words[last_words_data$element_id == 143,]
last_words[last_words_data$element_id == 15,]
last_words_dat %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
na.omit() %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
na.omit() %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
summary(last_words)
summary(last_words[,2])
str(last_words[,2])
last_words[,2] %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words[,2] %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words[,2]) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
last_words %>% unnest_tokens(tbl = ., output = word, input = last_words) %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
wordcloud2(shape = "cardioid",shuffle=F)
shiny::runApp('Documents/ND MS Data Science/R/HelloWorld')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
install.packages("shinythemes")
install.packages("shinythemes")
shiny::runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
runApp('Documents/ND MS Data Science/Data Viz/SouthBend')
shiny::runApp('Documents/ND MS Data Science/Data Viz/DataVizSouthBend')
